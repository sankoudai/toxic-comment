{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from scipy import sparse\n",
    "import xgboost, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "from importlib import reload \n",
    "from sklearn.pipeline import FeatureUnion, make_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from featurizer import CharGrpExtractorFeaturer, WordExtractorFeaturer\n",
    "import featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'featurizer' from '/Users/sankoudai/projects/toxic_classify/featurizer.py'>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(featurizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "subm = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVectorizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example with toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train[comment_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n"
     ]
    }
   ],
   "source": [
    "for t in train[comment_col]:\n",
    "    print(t)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_col = 'toxic'\n",
    "comment_col = 'comment_text'\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train[comment_col], train[label_col])\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(train[comment_col])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sankoudai/Library/Python/3.6/lib/python/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000, ngram_range=(1,1))\n",
    "tfidf_vect.fit(train[comment_col])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,2), max_features=5000)\n",
    "tfidf_vect_ngram.fit(train[comment_col])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# word level tf-idf\n",
    "tfidf_vect2 = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=6000, ngram_range=(1,1))\n",
    "tfidf_vect2.fit(train[comment_col])\n",
    "xtrain_tfidf2 =  tfidf_vect2.transform(train_x)\n",
    "xvalid_tfidf2 =  tfidf_vect2.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram2 = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}',  ngram_range=(1,2), max_features=6000)\n",
    "tfidf_vect_ngram2.fit(train[comment_col])\n",
    "xtrain_tfidf_ngram2 =  tfidf_vect_ngram2.transform(train_x)\n",
    "xvalid_tfidf_ngram2 =  tfidf_vect_ngram2.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sankoudai/Library/Python/3.6/lib/python/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', stop_words='english', max_df=0.9, ngram_range=(3,5), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(train[comment_col])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "badwords = dict((w,1) for w in pd.read_csv('data/badwords.txt', sep='\\t')['word'].values)\n",
    "bwFilter = WordExtractorFeaturer(badwords)\n",
    "counter = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "\n",
    "bwcFeaturizer = make_pipeline(bwFilter, counter)\n",
    "bwcFeaturizer.fit(train[comment_col])\n",
    "xtrain_bw_count = bwcFeaturizer.transform(train_x)\n",
    "xvalid_bw_count = bwcFeaturizer.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "badchars = dict((w,1) for w in pd.read_csv('data/bad_chargrps.txt', sep='\\t')['chargrp'].values)\n",
    "badcharFilter = CharGrpExtractorFeaturer(badchars)\n",
    "charCounter = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "bchf = make_pipeline(badcharFilter, charCounter)\n",
    "bchf.fit(train[comment_col])\n",
    "xtrain_bc_count = bchf.transform(train_x)\n",
    "xvalid_bc_count = bchf.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sankoudai/Library/Python/3.6/lib/python/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "uf = make_union(bchf, tfidf_vect, make_pipeline(tfidf_vect, nbfeaturer))\n",
    "xtrain_u = uf.transform(train_x)\n",
    "xvalid_u = uf.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119678, 15817)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class NBFeaturer(BaseEstimator):\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def preprocess_x(self, x, r):\n",
    "        return x.multiply(r)\n",
    "\n",
    "    def pr(self, x, y_i, y):\n",
    "        p = x[y==y_i].sum(0)\n",
    "        return (p+self.alpha) / (p.sum()+self.alpha)\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        self._r = sparse.csr_matrix(np.log(self.pr(x,1,y) / self.pr(x,0,y)))\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        x_nb = self.preprocess_x(x, self._r)\n",
    "        return x_nb\n",
    "\n",
    "nbfeaturer = NBFeaturer(0.2)\n",
    "nbfeaturer.fit(xtrain_tfidf, train_y)\n",
    "xtrain_nb = nbfeaturer.transform(xtrain_tfidf)\n",
    "xvalid_nb = nbfeaturer.transform(xvalid_tfidf)\n",
    "\n",
    "\n",
    "nbfeaturer2 = NBFeaturer(0.2)\n",
    "nbfeaturer2.fit(xtrain_tfidf2, train_y)\n",
    "xtrain_nb2 = nbfeaturer2.transform(xtrain_tfidf2)\n",
    "xvalid_nb2 = nbfeaturer2.transform(xvalid_tfidf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119678, 6000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sankoudai/Library/Python/3.6/lib/python/site-packages/ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# load the google word2vec model\n",
    "filename = 'data/GoogleNews-vectors-negative300.bin'\n",
    "goog_em_model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "\n",
    "tfidf_vect_inf = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=20000, ngram_range=(1,1))\n",
    "tfidf_vect_inf.fit(train[comment_col])\n",
    "def trans2wv_avg(wc_xs, em_model, count_vect):\n",
    "    wv_size = em_model.wv.vector_size\n",
    "    doc_cnt = wc_xs.get_shape()[0]\n",
    "    xs = np.zeros((doc_cnt, wv_size), dtype=np.float64)\n",
    "    words = count_vect.get_feature_names()\n",
    "    for doc_ind in range(doc_cnt):\n",
    "        doc_vec = wc_xs.getrow(doc_ind)\n",
    "        doc_em = np.zeros(wv_size)\n",
    "        doc_wc = 0\n",
    "        for word_ind, wc in zip(doc_vec.indices, doc_vec.data):\n",
    "            if word_ind >= len(words):continue\n",
    "            word = words[word_ind]\n",
    "            if word in em_model:\n",
    "                doc_em += em_model[word]*wc\n",
    "                doc_wc += wc\n",
    "        doc_em = doc_em / doc_wc\n",
    "        xs[doc_ind] = doc_em\n",
    "    return xs\n",
    "                \n",
    "xtrain_g300_avg = trans2wv_avg(xtrain_count, goog_em_model, count_vect)\n",
    "xvalid_g300_avg = trans2wv_avg(xvalid_count, goog_em_model, count_vect)\n",
    "xtrain_g300_avg_tfidf = trans2wv_avg(xtrain_count, goog_em_model, tfidf_vect_inf)\n",
    "xvalid_g300_avg_tfidf = trans2wv_avg(xvalid_count, goog_em_model, tfidf_vect_inf)\n",
    "\n",
    "def trans2wv_max(wc_xs, em_model):\n",
    "    wv_size = em_model.wv.vector_size\n",
    "    doc_cnt = wc_xs.get_shape()[0]\n",
    "    xs = np.zeros((doc_cnt, wv_size), dtype=np.float64)\n",
    "    words = count_vect.get_feature_names()\n",
    "    for doc_ind in range(doc_cnt):\n",
    "        doc_vec = wc_xs.getrow(doc_ind)\n",
    "        doc_em = np.zeros(wv_size)\n",
    "        for word_ind, wc in zip(doc_vec.indices, doc_vec.data):\n",
    "            if word_ind >= len(words):continue\n",
    "            word = words[word_ind]\n",
    "            if word in em_model:\n",
    "                doc_em = np.max(np.array([em_model[word], doc_em]), axis=0)\n",
    "        doc_em = doc_em\n",
    "        xs[doc_ind] = doc_em\n",
    "    return xs\n",
    "\n",
    "def trans2wv_max2(wc_xs, em_model, count_vect):\n",
    "    wv_size = em_model.wv.vector_size\n",
    "    doc_cnt = wc_xs.get_shape()[0]\n",
    "    xs = np.zeros((doc_cnt, wv_size), dtype=np.float64)\n",
    "    words = count_vect.get_feature_names()\n",
    "    for doc_ind in range(doc_cnt):\n",
    "        doc_vec = wc_xs.getrow(doc_ind)\n",
    "        doc_em = np.zeros(wv_size)\n",
    "        for word_ind, wc in zip(doc_vec.indices, doc_vec.data):\n",
    "            if word_ind >= len(words):continue\n",
    "            word = words[word_ind]\n",
    "            if word in em_model:\n",
    "                doc_em = np.max(np.array([em_model[word]*wc, doc_em]), axis=0)\n",
    "        doc_em = doc_em\n",
    "        xs[doc_ind] = doc_em\n",
    "    return xs\n",
    "\n",
    "xtrain_g300_max_tfidf = trans2wv_max2(xtrain_count, goog_em_model, tfidf_vect_inf)\n",
    "xvalid_g300_max_tfidf = trans2wv_max2(xvalid_count, goog_em_model, tfidf_vect_inf)\n",
    "xtrain_g300_max = trans2wv_max(xtrain_count, goog_em_model)\n",
    "xvalid_g300_max = trans2wv_max(xvalid_count, goog_em_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, valid_label=valid_y, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.940190008272128\n",
      "NB, WordLevel TF-IDF:  0.9488632090843006\n",
      "NB, WordLevel TF-IDF 6000:  0.9491890807911162\n",
      "NB, N-Gram Vectors:  0.9475597222570376\n",
      "NB, N-Gram Vectors 6000:  0.9484120021056326\n",
      "NB, N-Gram char:  0.9372320958564159\n",
      "NB, bad word count:  0.9132178577695335\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf2, train_y, xvalid_tfidf2)\n",
    "print(\"NB, WordLevel TF-IDF 6000: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"NB, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram2, train_y, xvalid_tfidf_ngram2)\n",
    "print(\"NB, N-Gram Vectors 6000: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"NB, N-Gram char: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_bw_count, train_y, xvalid_bw_count)\n",
    "print(\"NB, bad word count: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, bad word count:  0.9268042012383125\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_bc_count, train_y, xvalid_bc_count)\n",
    "print(\"NB, bad char count: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.9505427017271201\n",
      "LR, WordLevel TF-IDF:  0.9585390920712907\n",
      "LR, WordLevel TF-IDF 6000:  0.9586894943975134\n",
      "LR, N-Gram Vectors:  0.9560323866342466\n",
      "LR, N-Gram Vectors 6000:  0.9559571854711353\n",
      "LR, nb Vectors:  0.960544456420926\n",
      "LR, nb Vectors 6000:  0.9607951269646304\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf2, train_y, xvalid_tfidf2)\n",
    "print(\"LR, WordLevel TF-IDF 6000: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"LR, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram2, train_y, xvalid_tfidf_ngram2)\n",
    "print(\"LR, N-Gram Vectors 6000: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_nb, train_y, xvalid_nb)\n",
    "print(\"LR, nb Vectors: \", accuracy)\n",
    "\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_nb2, train_y, xvalid_nb2)\n",
    "print(\"LR, nb Vectors 6000: \", accuracy)\n",
    "\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "\n",
    "print(\"LR, nb chars 6000: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, badword count  train:  0.9227928274202444\n",
      "LR, badword count  valid:  0.9135437294763492\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(linear_model.LogisticRegression(),xtrain_bw_count, train_y, xtrain_bw_count, train_y)\n",
    "print(\"LR, badword count  train: \", accuracy)\n",
    "\n",
    "accuracy = train_model(linear_model.LogisticRegression(),xtrain_bw_count, train_y, xvalid_bw_count)\n",
    "print(\"LR, badword count  valid: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, badword count  valid:  0.9396385330759782\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(linear_model.LogisticRegression(),xtrain_bc_count, train_y, xvalid_bc_count)\n",
    "print(\"LR, badword count  valid: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, ultra  valid:  0.9556062467099491\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(linear_model.LogisticRegression(),xtrain_u, train_y, xvalid_u)\n",
    "print(\"LR, ultra  valid: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(xtrain_bw_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, g300_avg:  0.9440252675908054\n",
      "LR, g300_avg_tfidf:  0.9028150302058006\n",
      "LR, g300_max:  0.9395633319128669\n",
      "LR, g300_max_tfidf:  0.9033414383475797\n"
     ]
    }
   ],
   "source": [
    "xtrain_g300_avg[np.isnan(xtrain_g300_avg)] = 0 \n",
    "xvalid_g300_avg[np.isnan(xvalid_g300_avg)] = 0\n",
    "xtrain_g300_avg_tfidf[np.isnan(xtrain_g300_avg_tfidf)] = 0 \n",
    "xvalid_g300_avg_tfidf[np.isnan(xvalid_g300_avg_tfidf)] = 0\n",
    "xtrain_g300_max[np.isnan(xtrain_g300_max)] = 0 \n",
    "xvalid_g300_max[np.isnan(xvalid_g300_max)] = 0\n",
    "xtrain_g300_max_tfidf[np.isnan(xtrain_g300_max_tfidf)] = 0 \n",
    "xvalid_g300_max_tfidf[np.isnan(xvalid_g300_max_tfidf)] = 0\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_g300_avg, train_y, xvalid_g300_avg)\n",
    "print(\"LR, g300_avg: \", accuracy)\n",
    "\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_g300_avg_tfidf, train_y, xvalid_g300_avg_tfidf)\n",
    "print(\"LR, g300_avg_tfidf: \", accuracy)\n",
    "\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_g300_max, train_y, xvalid_g300_max)\n",
    "print(\"LR, g300_max: \", accuracy)\n",
    "\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_g300_max_tfidf, train_y, xvalid_g300_max_tfidf)\n",
    "print(\"LR, g300_max_tfidf: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = linear_model.LogisticRegression()\n",
    "lr.fit(xtrain_g300_avg, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_g300_avg[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_g300_avg[0] == xtrain_g300_avg[100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9062241496001805+0.09377585039981952"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2c0224615453>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# SVM on Ngram Level TF IDF Vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtrain_g300_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxvalid_g300_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SVM, word Vectors: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-b7a3189a72a1>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_vector_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_vector_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_neural_net\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# fit the training dataset on the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# predict the labels on validation dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sankoudai/Library/Python/3.6/lib/python/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sankoudai/Library/Python/3.6/lib/python/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    519\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    520\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    522\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/Users/sankoudai/Library/Python/3.6/lib/python/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sankoudai/Library/Python/3.6/lib/python/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 58\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_g300_avg, train_y, xvalid_g300_avg)\n",
    "print(\"SVM, word Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "print(\"Xgb, Count Vectors: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "print(\"Xgb, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g300_avg:  0.9449276815481413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sankoudai/Library/Python/3.6/lib/python/site-packages/sklearn/preprocessing/label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_g300_avg, train_y, xvalid_g300_avg)\n",
    "print(\"g300_avg: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 2 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1\n",
      " 1 1 2 1 1 1 1 1 1 1 1 1 1]\n",
      "[  1347   1931   8250   9236   9776  10494  14583  16167  36412  48592\n",
      "  50703  54515  57036  72714  81150  82385  94352  99480 101111 101375\n",
      " 101521 111959 116752 117322 117382 119640 120071 120099 128334 134929\n",
      " 148651 151286 154445 155917 157498 157823 162888 165753 166256 167792\n",
      " 168831 174476 179046 179475 179603 180801 181898 182849 185881 185954]\n"
     ]
    }
   ],
   "source": [
    "print(cv.data)\n",
    "print(cv.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-5ea092269bd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "len(xtrain_count.get_shape()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190339"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67,)\n",
      "[4987 4983 4916 4896 4870 4834 4825 4809 4685 4529 4472 4405 4266 4249\n",
      " 4197 4143 4070 3998 3609 3157 3156 3080 3076 2986 2779 2762 2709 2608\n",
      " 2320 2301 2093 1628 1568 1402  476  428  286  278  275  250   38 4948\n",
      " 4863 4826 4529 4483 4480 4470 4431 4278 3243 3101 2770 2642 2522 2424\n",
      " 2320 2099 1968 1256 1213 1180  702  694  476  455  444]\n",
      "(2, 5000)\n",
      "67\n",
      "[4987 4983 4916 4896 4870 4834 4825 4809 4685 4529 4472 4405 4266 4249\n",
      " 4197 4143 4070 3998 3609 3157 3156 3080 3076 2986 2779 2762 2709 2608\n",
      " 2320 2301 2093 1628 1568 1402  476  428  286  278  275  250   38]\n"
     ]
    }
   ],
   "source": [
    "i = xtrain_tfidf[0:2]\n",
    "print(i.data.shape)\n",
    "print(i.indices)\n",
    "print(i.shape)\n",
    "print(i.getnnz())\n",
    "print(i.getrow(0).indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4987)\t0.0746800166946854\n",
      "  (0, 4983)\t0.05142511992529977\n",
      "  (0, 4916)\t0.07179042201412006\n",
      "  (0, 4896)\t0.08189734307897052\n",
      "  (0, 4870)\t0.2045108638305972\n",
      "  (0, 4834)\t0.1958609889056362\n",
      "  (0, 4825)\t0.19226344567547962\n",
      "  (0, 4809)\t0.22614994704079744\n",
      "  (0, 4685)\t0.1009733781344479\n",
      "  (0, 4529)\t0.08974541162023222\n",
      "  (0, 4472)\t0.10707673173031468\n",
      "  (0, 4405)\t0.12067110382430597\n",
      "  (0, 4266)\t0.15904668705810476\n",
      "  (0, 4249)\t0.1963898778606448\n",
      "  (0, 4197)\t0.173885191407347\n",
      "  (0, 4143)\t0.1880355663776124\n",
      "  (0, 4070)\t0.16719762487808212\n",
      "  (0, 3998)\t0.09442547743100704\n",
      "  (0, 3609)\t0.17680994075753453\n",
      "  (0, 3157)\t0.26611943189090215\n",
      "  (0, 3156)\t0.05112122162733816\n",
      "  (0, 3080)\t0.0882276392834165\n",
      "  (0, 3076)\t0.18951909612520346\n",
      "  (0, 2986)\t0.23617933604339875\n",
      "  (0, 2779)\t0.16301250581842372\n",
      "  (0, 2762)\t0.13558338613184528\n",
      "  (0, 2709)\t0.14400162515240644\n",
      "  (0, 2608)\t0.2024082270061789\n",
      "  (0, 2320)\t0.047939090005863834\n",
      "  (0, 2301)\t0.09965399084632623\n",
      "  (0, 2093)\t0.11767288799617458\n",
      "  (0, 1628)\t0.11970714629248722\n",
      "  (0, 1568)\t0.14385775027055983\n",
      "  (0, 1402)\t0.2003234888220986\n",
      "  (0, 476)\t0.14879729378540066\n",
      "  (0, 428)\t0.08771767132795012\n",
      "  (0, 286)\t0.13552248685403875\n",
      "  (0, 278)\t0.09786882275711821\n",
      "  (0, 275)\t0.2037474723288492\n",
      "  (0, 250)\t0.19607140639670614\n",
      "  (0, 38)\t0.2262968716521036\n"
     ]
    }
   ],
   "source": [
    "for i in xtrain_tfidf[0:1].:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.word_vec('curse'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(train[comment_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,    121,    154,      5,\n",
       "           250,   3637,    212,      5,    702,  25333,     64,    253,\n",
       "             7,    477,     70,      6,   6794,     21,     46,   1179,\n",
       "           300,      2,    885,      3,   2243,    593,     76,     42,\n",
       "          1376,  17730,   1587,     28,    126,      4,   1035,    334,\n",
       "            20,   1494,   1449,   8799,      4,   4376,  29790,      2,\n",
       "          7331,     88,    212,      4,   1955,  25685,   5512,   1668,\n",
       "          2155,   5169,   1754,   3881],\n",
       "       [     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,    259,\n",
       "         95278,     47,   3908, 202393,     16,    408,      1,    259,\n",
       "         95278,     12,      1,   5876,     72,      4,     12,      1,\n",
       "         29466,     51,   2513,   2344,    152,     41,      4,     86,\n",
       "          2552,      2,   8442,  95279,     65,      7,   2043,      1,\n",
       "          7950,      4,   8445,    102,     12,      1,  29466,     68,\n",
       "            11,     24,     14,    168,      2,  62854,     11,     24,\n",
       "           168,      2,   8442,  95279]], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence.pad_sequences(token.texts_to_sequences(train_x[0:2]), maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136467    Go take a long walk off a short pier. \\n\\nSee ...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.index_word[121]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1347"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect.get_feature_names().index('damn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sankoudai/Library/Python/3.6/lib/python/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "def trans2wv_avg(wc_xs, em_model, count_vect):\n",
    "    wv_size = em_model.wv.vector_size\n",
    "    doc_cnt = wc_xs.get_shape()[0]\n",
    "    xs = np.zeros((doc_cnt, wv_size), dtype=np.float64)\n",
    "    words = count_vect.get_feature_names()\n",
    "    for doc_ind in range(doc_cnt):\n",
    "        doc_vec = wc_xs.getrow(doc_ind)\n",
    "        doc_em = np.zeros(wv_size)\n",
    "        doc_wc = 0\n",
    "        for word_ind, wc in zip(doc_vec.indices, doc_vec.data):\n",
    "            if word_ind >= len(words):continue\n",
    "            word = words[word_ind]\n",
    "            if word in em_model:\n",
    "                doc_em += em_model[word]*wc\n",
    "                doc_wc += wc\n",
    "        doc_em = doc_em / doc_wc\n",
    "        xs[doc_ind] = doc_em\n",
    "    return xs\n",
    "                \n",
    "xtrain_g300_avg = trans2wv_avg(xtrain_count, goog_em_model, count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sankoudai/Library/Python/3.6/lib/python/site-packages/scipy/sparse/compressed.py:226: SparseEfficiencyWarning: Comparing sparse matrices using == is inefficient, try using != instead.\n",
      "  \" != instead.\", SparseEfficiencyWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "190285"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xtrain_count[0,:]==xtrain_count[10,:]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "t = xtrain_g300_avg[0]\n",
    "for i in range(len(xtrain_g300_avg)):\n",
    "    t2 = xtrain_g300_avg[i]\n",
    "    if (t != t2).sum() != 0:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextFeaturer(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        df = pd.DataFrame({\"text\":x})\n",
    "        df['char_count'] = df['text'].apply(len)\n",
    "        df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "        df['word_density'] = df['char_count'] / (df['word_count']+1)\n",
    "        df['punctuation_count'] = df['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "        df['title_word_count'] = df['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "        df['upper_case_word_count'] = df['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
    "        \n",
    "        pos_family = {\n",
    "            'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "            'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "            'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "            'adj' :  ['JJ','JJR','JJS'],\n",
    "            'adv' : ['RB','RBR','RBS','WRB']\n",
    "        }\n",
    "\n",
    "        # function to check and get the part of speech tag count of a words in a given sentence\n",
    "        def check_pos_tag(x, flag):\n",
    "            cnt = 0\n",
    "            try:\n",
    "                wiki = textblob.TextBlob(x)\n",
    "                for tup in wiki.tags:\n",
    "                    ppo = list(tup)[1]\n",
    "                    if ppo in pos_family[flag]:\n",
    "                        cnt += 1\n",
    "            except:\n",
    "                pass\n",
    "            return cnt\n",
    "\n",
    "        df['noun_count'] = df['text'].apply(lambda x: check_pos_tag(x, 'noun'))/df['word_count']\n",
    "        df['verb_count'] = df['text'].apply(lambda x: check_pos_tag(x, 'verb'))/df['word_count']\n",
    "        df['adj_count'] = df['text'].apply(lambda x: check_pos_tag(x, 'adj'))/df['word_count']\n",
    "        df['adv_count'] = df['text'].apply(lambda x: check_pos_tag(x, 'adv'))/df['word_count']\n",
    "        df['pron_count'] = df['text'].apply(lambda x: check_pos_tag(x, 'pron'))/df['word_count']\n",
    "        df['noun_count'] = df['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "#         df = df.drop(\"text\", axis=1)\n",
    "#         return df.values\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_featurer = TextFeaturer()\n",
    "t = text_featurer.transform(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = pd.DataFrame({\"text\":train_x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
